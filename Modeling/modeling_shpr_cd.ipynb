{"cells":[{"cell_type":"markdown","metadata":{},"source":["#### download package"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3040,"status":"ok","timestamp":1635358361998,"user":{"displayName":"이유림","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09001258585881898886"},"user_tz":-540},"id":"X11eBgt8m_7G","outputId":"38a6a570-7aa4-4352-810b-52e7d7d3c67d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sktime in /usr/local/lib/python3.7/dist-packages (0.8.0)\n","Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.1.5)\n","Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.0.1)\n","Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.19.5)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from sktime) (0.37.0)\n","Requirement already satisfied: statsmodels>=0.12.1 in /usr/local/lib/python3.7/dist-packages (from sktime) (0.13.0)\n","Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.7/dist-packages (from sktime) (0.54.1)\n","Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->sktime) (0.37.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->sktime) (57.4.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->sktime) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->sktime) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->sktime) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->sktime) (3.0.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->sktime) (1.0.1)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->sktime) (1.4.1)\n","Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.12.1->sktime) (0.5.2)\n"]}],"source":["# download sktime package \n","!pip install sktime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aqS4t0ovUqTb"},"outputs":[],"source":["# 필요한 패키지 import\n","import os\n","import sys\n","import warnings\n","import plotly\n","import numpy as np\n","import pandas as pd\n","import datetime\n","import tensorflow as tf\n","from tqdm import tqdm\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import KFold, train_test_split, TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from sklearn import preprocessing\n","from sklearn.preprocessing import RobustScaler, StandardScaler\n","from sktime.forecasting.model_selection import temporal_train_test_split\n","from sktime.utils.plotting import plot_series\n","from scipy.stats import reciprocal \n","\n","from xgboost import XGBRegressor\n","from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n","from sklearn.svm import SVR\n","import lightgbm as lgb\n","from lightgbm.sklearn import LGBMRegressor\n","from sklearn.multioutput import MultiOutputRegressor\n","\n","mpl.rcParams['figure.figsize'] = (8, 6)\n","mpl.rcParams['axes.grid'] = False\n","\n","warnings.filterwarnings('ignore')\n","pd.options.display.float_format = '{:.5f}'.format"]},{"cell_type":"markdown","metadata":{},"source":["## Model training"]},{"cell_type":"markdown","metadata":{},"source":["### making datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6x6993ZHqvAf"},"outputs":[],"source":["shpr_df = ['90001302', '90001441', '90001542', '90001341', '90001541',\n","       '90001443', '90001381', '90001521', '90001582', '90001602',\n","       '90001662', '90001622', '90001682', '90001683', '90001702',\n","       '90001703', '90001705', '90001704', '90001664', '90001768',\n","       '90001765', '90001776', '90001774', '90001842']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXYZvSEhoFNQ"},"outputs":[],"source":["# 학습에 필요한 train 및 test dataset 만드는 과정\n","def get_train_test_set(shpr_cd):\n","  df = pd.read_pickle(\"shpr_cd_\"+shpr_cd+\".pkl\")\n","  train = df[df['BKG_DATE'] <= '2021-06-20']\n","  test = df[df['BKG_DATE'] > '2021-06-20']\n","  # 나머지 Scaling\n","  scaling_features = ['DAY_1', 'DAY_2', 'DAY_3', 'WEEK_AMT',\n","       '100이상 27307.5미만','27307.5이상 63200미만','63200이상 133375미만', '133375이상 290100미만',\n","       '290100이상 1170901미만','MEAN_PRICE', '강원', '경기', '경남', '경북', '광주',\n","       '대구', '대전', '부산', '서울', '세종', '울산', '인천', '전남', '전북', '제주', '충남', '충북',\n","       '0.0', '1.0', '10', '2.0', '3.0', '4.0', '5.0', '6.0', '7.0', '8.0', '9.0']\n","\n","  scaler = StandardScaler()\n","  train.loc[:, scaling_features] = scaler.fit_transform(train[scaling_features])\n","  test.loc[:, scaling_features] = scaler.transform(test[scaling_features])\n","  train_x = train.drop(['ITEM_QTY','BKG_DATE'], axis=1)\n","  train_y = train['ITEM_QTY']\n","\n","  test_x = test.drop(['ITEM_QTY','BKG_DATE'], axis=1)\n","  test_y = test['ITEM_QTY']\n","  return train_x, train_y, test_x, test_y"]},{"cell_type":"markdown","metadata":{},"source":["### 모델 별 정의 및 파라미터 최적화"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fw_LCffCKoVE"},"outputs":[],"source":["# 모델별 학습 파라미터 정의\n","XGBRegressor_param = {'n_estimators' : [100], 'eta' : [0.01], 'min_child_weight' : np.arange(1, 8, 1), 'max_depth' : np.arange(3,9,1) , 'colsample_bytree' :np.arange(0.8, 1.0, 0.1), 'subsample' :np.arange(0.8, 1.0, 0.1)}\n","LGBMRegressor_param = {'max_depth' : range(3,15,3), 'min_child_weight': range(1,6,2), 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100], 'learning_rate':[0.1, 0.01], 'max_depth' : [6,8,10]}\n","SVR_param = {'kernel':['linear'], 'C':[1.0], 'epsilon':[0.1]}\n","GradientBoostingRegressor_param = {'n_estimators':[100], 'max_depth':np.arange(3,20,3)}\n","AdaBoostRegressor_param = {'n_estimators' : np.arange(25, 100, 25), 'loss': ['linear', 'square', 'exponential'], 'learning_rate': np.arange(0.1, 1)} "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_YiraP3Lp5F"},"outputs":[],"source":["# 모델별 정의\n","XGBRegressor_model = XGBRegressor(n_estimators = 100, objective = 'reg:squarederror')\n","LGBMRegressor_model = LGBMRegressor(n_estimators = 80)\n","SVR_model = SVR(kernel='linear', C=1.0, epsilon=0.1)\n","GradientBoostingRegressor_model = GradientBoostingRegressor(n_estimators=100, max_depth=3)\n","AdaBoostRegressor_model = AdaBoostRegressor(base_estimator=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOwgwkfxOntj"},"outputs":[],"source":["# 단일 모델별 최적 파라미터로 모델링\n","def print_best_params(model, params, x_train, x_test, y_train, y_test, log=False):\n","\n","  tss = TimeSeriesSplit(n_splits=5)\n","  grid_model = GridSearchCV(model, cv = tss, param_grid=params, scoring='neg_mean_absolute_error')\n","  grid_model.fit(x_train, y_train)\n","  mae = -1 * grid_model.best_score_\n","  #print('{0} 최적 평균 mae값 : {1}, 최적 파라미터:{2}'.format(model.__class__.__name__, np.round(mae, 4), grid_model.best_params_))\n","\n","  best_model = grid_model.best_estimator_\n","  pred = best_model.predict(x_test)\n","\n","  if log:\n","    y_test = np.expm1(y_test)\n","    pred = np.expm1(pred)\n","  \n","  single_min_list = np.round(mean_absolute_error(y_test, pred), 4)\n","\n","  return best_model, single_min_list, pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UoKJha9-U3fw"},"outputs":[],"source":["# 단일 모델에서의 MAE 값이 가장 작은 세 개의 모델로 stacking, stacking model의 dataset 만드는 함수\n","def get_stacking_base_datasets(model, x_train_n, y_train_n, x_test_n, n_splits=5):\n","  # 지정된 n_folds 값으로 KFold 생성\n","  tss = TimeSeriesSplit(n_splits)\n","\n","  # 추후 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화\n","  train_fold_pred = np.zeros((x_train_n.shape[0], 1))\n","  test_pred = np.zeros((x_test_n.shape[0], n_splits))\n","  #print(model.__class__.__name__, ' model 시작')\n","\n","  for folder_counter, (train_index, valid_index) in enumerate(tss.split(x_train_n)):\n","    # 입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 세트 추출\n","    #print('\\t 폴드 세트: ', folder_counter, ' 시작')\n","    x_tr = x_train_n[train_index]\n","    y_tr = y_train_n[train_index]\n","    x_te = x_train_n[valid_index]\n","\n","    # 폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행\n","    model.fit(x_tr, y_tr)\n","    # 폴드 세트 내부에서 다시 만들어지 검증 데이터로 기반 모델 예측 후 데이터 저장\n","    train_fold_pred[valid_index, :]=model.predict(x_te).reshape(-1, 1)\n","    # 입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장\n","    test_pred[:, folder_counter] = model.predict(x_test_n)\n","\n","  # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성\n","  test_pred_mean = np.mean(test_pred, axis=1).reshape(-1, 1)\n","\n","  # train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터\n","  return train_fold_pred, test_pred_mean"]},{"cell_type":"markdown","metadata":{},"source":["### 모델 학습 과정\n","    - 단일 모델 학습 및 stacking 모델 학습\n","    - 모델별 성능 비교 후 최적의 모델 반환"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"betSsoZEFiV3"},"outputs":[],"source":["import sys\n","mod = sys.modules[__name__]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBoibHKPO26R"},"outputs":[],"source":["# 최적의 파라미터로 각각의 모델 학습\n","def get_optimal_model(shpr_cd):\n","  x_train, y_train, x_test, y_test = get_train_test_set(shpr_cd)\n","  x_train_n=x_train.values\n","  x_test_n=x_test.values\n","  y_train_n=y_train.values\n","  \n","  # 단일 모델별 성능 리스트\n","  single_min_list = dict()\n","  \n","  # 단일 모델 학습\n","  globals()[\"XGBRegressor_model_tuned_{}\".format(shpr_cd)], single_min_list[\"XGBRegressor\"], globals()[\"single_pred_XGBRegressor_{}\".format(shpr_cd)] = print_best_params(XGBRegressor_model, XGBRegressor_param, x_train, x_test, y_train, y_test)\n","  globals()[\"LGBMRegressor_model_tuned_{}\".format(shpr_cd)], single_min_list[\"LGBMRegressor\"], globals()[\"single_pred_LGBMRegressor_{}\".format(shpr_cd)] = print_best_params(LGBMRegressor_model, LGBMRegressor_param, x_train, x_test, y_train, y_test)\n","  globals()[\"SVR_model_tuned_{}\".format(shpr_cd)], single_min_list[\"SVR\"], globals()[\"single_pred_SVR_{}\".format(shpr_cd)] = print_best_params(SVR_model, SVR_param, x_train, x_test, y_train, y_test)\n","  globals()[\"GradientBoostingRegressor_model_tuned_{}\".format(shpr_cd)], single_min_list[\"GradientBoostingRegressor\"], globals()[\"single_pred_GradientBoostingRegressor_{}\".format(shpr_cd)] = print_best_params(GradientBoostingRegressor_model, GradientBoostingRegressor_param, x_train, x_test, y_train, y_test)\n","  globals()[\"AdaBoostRegressor_model_tuned_{}\".format(shpr_cd)], single_min_list[\"AdaBoostRegressor\"], globals()[\"single_pred_AdaBoostRegressor_{}\".format(shpr_cd)] = print_best_params(AdaBoostRegressor_model, AdaBoostRegressor_param, x_train, x_test, y_train, y_test)\n","\n","  single_model_mae = sorted(single_min_list.items(), key = lambda item: item[1])\n","  \n","  # Stacking 모델별 성능 리스트\n","  stacking_list = dict()\n","\n","# stacking model dataset 생성\n","  globals()[\"{}_train_{}\".format(single_model_mae[0][0], shpr_cd)], globals()[\"{}_test_{}\".format(single_model_mae[0][0], shpr_cd)] = get_stacking_base_datasets(getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[0][0], shpr_cd)), x_train_n, y_train_n, x_test_n, 5)                                                                                                          \n","  globals()[\"{}_train_{}\".format(single_model_mae[1][0], shpr_cd)], globals()[\"{}_test_{}\".format(single_model_mae[1][0], shpr_cd)] = get_stacking_base_datasets(getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[1][0], shpr_cd)), x_train_n, y_train_n, x_test_n, 5) \n","                                                                                                            \n","  # 첫번째 경우\n","  stack_final_x_train = np.concatenate((getattr(mod, \"{}_train_{}\".format(single_model_mae[0][0], shpr_cd)), getattr(mod, \"{}_train_{}\".format(single_model_mae[1][0], shpr_cd))), axis=1)\n","  stack_final_x_test = np.concatenate((getattr(mod, \"{}_test_{}\".format(single_model_mae[0][0], shpr_cd)), getattr(mod, \"{}_test_{}\".format(single_model_mae[1][0], shpr_cd))), axis=1)\n","\n","  globals()[\"meta_model_{}_{}\".format(single_model_mae[2][0], shpr_cd)] = getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[2][0], shpr_cd))\n","\n","  getattr(mod, \"meta_model_{}_{}\".format(single_model_mae[2][0], shpr_cd)).fit(stack_final_x_train, y_train)\n","  globals()[\"stack_pred_{}_{}\".format(single_model_mae[2][0], shpr_cd)] = getattr(mod, \"meta_model_{}_{}\".format(single_model_mae[2][0], shpr_cd)).predict(stack_final_x_test)\n","  stacking_list[\"{}\".format(single_model_mae[2][0])] =  mean_absolute_error(y_test, getattr(mod, \"stack_pred_{}_{}\".format(single_model_mae[2][0], shpr_cd)))\n","\n","  # 두번째 경우\n","  globals()[\"{}_train_{}\".format(single_model_mae[0][0], shpr_cd)], globals()[\"{}_test_{}\".format(single_model_mae[0][0], shpr_cd)] = get_stacking_base_datasets(getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[0][0], shpr_cd)), x_train_n, y_train_n, x_test_n, 5)\n","                                                                                                            \n","  globals()[\"{}_train_{}\".format(single_model_mae[2][0], shpr_cd)], globals()[\"{}_test_{}\".format(single_model_mae[2][0], shpr_cd)] = get_stacking_base_datasets(getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[2][0], shpr_cd)), x_train_n, y_train_n, x_test_n, 5) \n","                                                                                                            \n","\n","  stack_final_x_train = np.concatenate((getattr(mod, \"{}_train_{}\".format(single_model_mae[0][0], shpr_cd)), getattr(mod, \"{}_train_{}\".format(single_model_mae[2][0], shpr_cd))), axis=1)\n","  stack_final_x_test = np.concatenate((getattr(mod, \"{}_test_{}\".format(single_model_mae[0][0], shpr_cd)), getattr(mod, \"{}_test_{}\".format(single_model_mae[2][0], shpr_cd))), axis=1)\n","\n","  globals()[\"meta_model_{}_{}\".format(single_model_mae[1][0], shpr_cd)] = getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[1][0], shpr_cd))\n","\n","  getattr(mod, \"meta_model_{}_{}\".format(single_model_mae[1][0], shpr_cd)).fit(stack_final_x_train, y_train)\n","  globals()[\"stack_pred_{}_{}\".format(single_model_mae[1][0], shpr_cd)] = getattr(mod, \"meta_model_{}_{}\".format(single_model_mae[1][0], shpr_cd)).predict(stack_final_x_test)\n","  stacking_list[\"{}\".format(single_model_mae[1][0])] =  mean_absolute_error(y_test, getattr(mod, \"stack_pred_{}_{}\".format(single_model_mae[1][0], shpr_cd)))\n","\n","\n","  # 세번째 경우\n","  globals()[\"{}_train_{}\".format(single_model_mae[1][0], shpr_cd)], globals()[\"{}_test_{}\".format(single_model_mae[1][0], shpr_cd)] = get_stacking_base_datasets(getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[1][0], shpr_cd)), x_train_n, y_train_n, x_test_n, 5)\n","                                                                                                            \n","  globals()[\"{}_train_{}\".format(single_model_mae[2][0], shpr_cd)], globals()[\"{}_test_{}\".format(single_model_mae[2][0], shpr_cd)] = get_stacking_base_datasets(getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[2][0], shpr_cd)), x_train_n, y_train_n, x_test_n, 5) \n","                                                                                                            \n","\n","  stack_final_x_train = np.concatenate((getattr(mod, \"{}_train_{}\".format(single_model_mae[1][0], shpr_cd)), getattr(mod, \"{}_train_{}\".format(single_model_mae[2][0], shpr_cd))), axis=1)\n","  stack_final_x_test = np.concatenate((getattr(mod, \"{}_test_{}\".format(single_model_mae[1][0], shpr_cd)), getattr(mod, \"{}_test_{}\".format(single_model_mae[2][0], shpr_cd))), axis=1)\n","\n","  globals()[\"meta_model_{}_{}\".format(single_model_mae[0][0], shpr_cd)] = getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[0][0], shpr_cd))\n","\n","  getattr(mod, \"meta_model_{}_{}\".format(single_model_mae[0][0], shpr_cd)).fit(stack_final_x_train, y_train)\n","  globals()[\"stack_pred_{}_{}\".format(single_model_mae[0][0], shpr_cd)] = getattr(mod, \"meta_model_{}_{}\".format(single_model_mae[0][0], shpr_cd)).predict(stack_final_x_test)\n","  stacking_list[\"{}\".format(single_model_mae[0][0])] =  mean_absolute_error(y_test, getattr(mod, \"stack_pred_{}_{}\".format(single_model_mae[0][0], shpr_cd)))\n","\n","  stacking_model_mae = sorted(stacking_list.items(), key = lambda item: item[1])\n","\n","# 단일 모델과 stacking 모델의 MAE 값을 비교하여 작은 값으로 모델링 결과 반환\n","  a = single_model_mae[0][1]\n","  b = stacking_model_mae[0][1]\n","\n","  if a < b:\n","    print(\"쇼핑몰 코드 {}의 최적 모델은 단일 모델 {} : (test MAE값) {}\".format(shpr_cd, single_model_mae[0][0], single_model_mae[0][1]))\n","    globals()[\"best_model_{}\".format(shpr_cd)]  = single_model_mae[0][0]\n","    globals()[\"test_mae_{}\".format(shpr_cd)] = single_model_mae[0][1]\n","    globals()[\"best_pred_{}\".format(shpr_cd)] = getattr(mod, \"single_pred_{}_{}\".format(single_model_mae[0][0], shpr_cd))\n","  else:\n","    print(\"쇼핑몰 코드 {}의 최적 모델은 stacking meta 모델 {} : (test MAE값) {}\".format(shpr_cd, stacking_model_mae[0][0], stacking_model_mae[0][1]))\n","    globals()[\"best_model_{}\".format(shpr_cd)] = stacking_model_mae[0][0]\n","    globals()[\"test_mae_{}\".format(shpr_cd)] = stacking_model_mae[0][1]\n","    globals()[\"best_pred_{}\".format(shpr_cd)] = getattr(mod, \"stack_pred_{}_{}\".format(stacking_model_mae[0][0], shpr_cd))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":690762,"status":"ok","timestamp":1635363992721,"user":{"displayName":"이유림","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09001258585881898886"},"user_tz":-540},"id":"LX9-RARbgAEv","outputId":"9c7eb157-8c20-43e2-b575-1b3a8d93770b"},"outputs":[{"name":"stdout","output_type":"stream","text":["쇼핑몰 코드 90001302의 최적 모델은 stacking meta 모델 SVR : (test MAE값) 60.218552047014235\n","쇼핑몰 코드 90001441의 최적 모델은 stacking meta 모델 GradientBoostingRegressor : (test MAE값) 30.406663026077627\n","쇼핑몰 코드 90001542의 최적 모델은 단일 모델 GradientBoostingRegressor : (test MAE값) 30.0344\n","쇼핑몰 코드 90001341의 최적 모델은 단일 모델 GradientBoostingRegressor : (test MAE값) 30.2651\n","쇼핑몰 코드 90001541의 최적 모델은 단일 모델 GradientBoostingRegressor : (test MAE값) 34.5527\n","쇼핑몰 코드 90001443의 최적 모델은 단일 모델 GradientBoostingRegressor : (test MAE값) 139.5502\n","쇼핑몰 코드 90001381의 최적 모델은 stacking meta 모델 AdaBoostRegressor : (test MAE값) 0.0\n","쇼핑몰 코드 90001521의 최적 모델은 단일 모델 SVR : (test MAE값) 29.3362\n","쇼핑몰 코드 90001582의 최적 모델은 단일 모델 GradientBoostingRegressor : (test MAE값) 142.9214\n","쇼핑몰 코드 90001602의 최적 모델은 단일 모델 GradientBoostingRegressor : (test MAE값) 88.0169\n","쇼핑몰 코드 90001662의 최적 모델은 단일 모델 XGBRegressor : (test MAE값) 14.6649\n","쇼핑몰 코드 90001622의 최적 모델은 단일 모델 XGBRegressor : (test MAE값) 136.0799\n","쇼핑몰 코드 90001682의 최적 모델은 단일 모델 GradientBoostingRegressor : (test MAE값) 35.9206\n","쇼핑몰 코드 90001683의 최적 모델은 단일 모델 AdaBoostRegressor : (test MAE값) 9.2927\n","쇼핑몰 코드 90001702의 최적 모델은 단일 모델 XGBRegressor : (test MAE값) 234.0701\n","쇼핑몰 코드 90001703의 최적 모델은 단일 모델 GradientBoostingRegressor : (test MAE값) 50.4392\n","쇼핑몰 코드 90001705의 최적 모델은 단일 모델 AdaBoostRegressor : (test MAE값) 67.9028\n","쇼핑몰 코드 90001704의 최적 모델은 단일 모델 SVR : (test MAE값) 36.9082\n","쇼핑몰 코드 90001664의 최적 모델은 단일 모델 GradientBoostingRegressor : (test MAE값) 5.0687\n","쇼핑몰 코드 90001768의 최적 모델은 단일 모델 GradientBoostingRegressor : (test MAE값) 157.3011\n","쇼핑몰 코드 90001765의 최적 모델은 stacking meta 모델 XGBRegressor : (test MAE값) 721.299985218048\n","쇼핑몰 코드 90001776의 최적 모델은 단일 모델 SVR : (test MAE값) 2074.7152\n","쇼핑몰 코드 90001774의 최적 모델은 stacking meta 모델 SVR : (test MAE값) 84.3\n","쇼핑몰 코드 90001842의 최적 모델은 stacking meta 모델 SVR : (test MAE값) 6.2\n"]}],"source":["# 단일 모델과 stacking 모델의 MAE 값을 비교하여 작은 값으로 모델링 결과 반환\n","for shpr_cd in shpr_df:\n","  get_optimal_model(shpr_cd)"]},{"cell_type":"markdown","metadata":{},"source":["### 모델 학습 결과에 대한 예측"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpk5wzBXjcfx"},"outputs":[],"source":["for shpr_cd in shpr_df:\n","  x_train, y_train, x_test, y_test = get_train_test_set(shpr_cd)\n","  globals()['pred_{}'.format(shpr_cd)]= getattr(mod, \"stack_pred_{}_{}\".format(getattr(mod, \"best_model_{}\".format(shpr_cd)), shpr_cd))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nf4y3SqpoyVr"},"outputs":[],"source":["# test dataset에 대한 예측값 \n","shpr_pred_df = pd.DataFrame()\n","for shpr_cd in shpr_df:\n","  shpr_pred_df[str(shpr_cd)] = getattr(mod, 'best_pred_{}'.format(shpr_cd))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":379},"executionInfo":{"elapsed":321,"status":"ok","timestamp":1635364306737,"user":{"displayName":"이유림","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09001258585881898886"},"user_tz":-540},"id":"8srdGc-yxfor","outputId":"294e1fac-9584-4324-b1d3-9128cdd53acb"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>90001302</th>\n","      <th>90001441</th>\n","      <th>90001542</th>\n","      <th>90001341</th>\n","      <th>90001541</th>\n","      <th>90001443</th>\n","      <th>90001381</th>\n","      <th>90001521</th>\n","      <th>90001582</th>\n","      <th>90001602</th>\n","      <th>90001662</th>\n","      <th>90001622</th>\n","      <th>90001682</th>\n","      <th>90001683</th>\n","      <th>90001702</th>\n","      <th>90001703</th>\n","      <th>90001705</th>\n","      <th>90001704</th>\n","      <th>90001664</th>\n","      <th>90001768</th>\n","      <th>90001765</th>\n","      <th>90001776</th>\n","      <th>90001774</th>\n","      <th>90001842</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3226.09386</td>\n","      <td>1184.94575</td>\n","      <td>1416.28051</td>\n","      <td>1477.83446</td>\n","      <td>2389.53713</td>\n","      <td>1776.70269</td>\n","      <td>0.00000</td>\n","      <td>658.68083</td>\n","      <td>1751.88053</td>\n","      <td>2932.72793</td>\n","      <td>174.40498</td>\n","      <td>2634.38721</td>\n","      <td>1450.75144</td>\n","      <td>109.50000</td>\n","      <td>1353.64319</td>\n","      <td>2611.08004</td>\n","      <td>1406.90000</td>\n","      <td>891.76732</td>\n","      <td>69.85862</td>\n","      <td>210.30665</td>\n","      <td>0.00001</td>\n","      <td>28.17773</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3011.82735</td>\n","      <td>2050.50788</td>\n","      <td>1238.59269</td>\n","      <td>1424.99268</td>\n","      <td>2135.31718</td>\n","      <td>1871.27577</td>\n","      <td>0.00000</td>\n","      <td>847.62429</td>\n","      <td>1631.23147</td>\n","      <td>2215.02971</td>\n","      <td>123.79654</td>\n","      <td>1464.72363</td>\n","      <td>1330.24467</td>\n","      <td>130.41667</td>\n","      <td>1361.34961</td>\n","      <td>2072.81506</td>\n","      <td>1082.36364</td>\n","      <td>875.11641</td>\n","      <td>61.64254</td>\n","      <td>110.79168</td>\n","      <td>0.00001</td>\n","      <td>36.07948</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2794.30549</td>\n","      <td>890.04308</td>\n","      <td>1559.96406</td>\n","      <td>1334.68888</td>\n","      <td>1793.24514</td>\n","      <td>1871.52703</td>\n","      <td>0.00000</td>\n","      <td>558.44538</td>\n","      <td>1539.70471</td>\n","      <td>2446.65606</td>\n","      <td>184.56906</td>\n","      <td>1546.18298</td>\n","      <td>1263.98054</td>\n","      <td>107.22727</td>\n","      <td>1316.06238</td>\n","      <td>2026.09197</td>\n","      <td>960.00000</td>\n","      <td>999.82339</td>\n","      <td>41.10904</td>\n","      <td>103.41867</td>\n","      <td>0.00001</td>\n","      <td>28.56960</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3186.88129</td>\n","      <td>809.55347</td>\n","      <td>1261.85854</td>\n","      <td>1129.26486</td>\n","      <td>1584.23225</td>\n","      <td>1515.24092</td>\n","      <td>0.00000</td>\n","      <td>922.16851</td>\n","      <td>1450.13645</td>\n","      <td>1999.05078</td>\n","      <td>199.51991</td>\n","      <td>1285.97253</td>\n","      <td>1296.23010</td>\n","      <td>108.00000</td>\n","      <td>1167.81750</td>\n","      <td>1932.93337</td>\n","      <td>1092.75000</td>\n","      <td>793.45183</td>\n","      <td>54.07507</td>\n","      <td>110.12193</td>\n","      <td>0.00001</td>\n","      <td>26.11894</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3860.56491</td>\n","      <td>701.91556</td>\n","      <td>1238.17586</td>\n","      <td>1241.06078</td>\n","      <td>1156.71312</td>\n","      <td>1241.10308</td>\n","      <td>0.00000</td>\n","      <td>577.27586</td>\n","      <td>2558.92395</td>\n","      <td>1595.77271</td>\n","      <td>138.92957</td>\n","      <td>2669.20337</td>\n","      <td>869.64703</td>\n","      <td>83.84000</td>\n","      <td>1111.61829</td>\n","      <td>1434.33167</td>\n","      <td>856.75000</td>\n","      <td>1329.68656</td>\n","      <td>39.63033</td>\n","      <td>103.14280</td>\n","      <td>0.00001</td>\n","      <td>33.97936</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2176.74400</td>\n","      <td>690.30646</td>\n","      <td>1140.97781</td>\n","      <td>867.23572</td>\n","      <td>984.39306</td>\n","      <td>4220.84825</td>\n","      <td>0.00000</td>\n","      <td>345.66180</td>\n","      <td>1065.20288</td>\n","      <td>1486.88818</td>\n","      <td>104.92847</td>\n","      <td>1041.50696</td>\n","      <td>926.40427</td>\n","      <td>107.22727</td>\n","      <td>925.45831</td>\n","      <td>1303.00698</td>\n","      <td>861.66667</td>\n","      <td>601.43248</td>\n","      <td>35.05866</td>\n","      <td>96.64119</td>\n","      <td>0.00001</td>\n","      <td>13.44701</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2773.45692</td>\n","      <td>901.28155</td>\n","      <td>2095.44362</td>\n","      <td>1333.95020</td>\n","      <td>1359.39210</td>\n","      <td>2860.46988</td>\n","      <td>0.00000</td>\n","      <td>419.00848</td>\n","      <td>2191.98755</td>\n","      <td>2431.02847</td>\n","      <td>160.56714</td>\n","      <td>1664.49744</td>\n","      <td>1442.41318</td>\n","      <td>246.20000</td>\n","      <td>1416.61011</td>\n","      <td>1744.96246</td>\n","      <td>1084.85714</td>\n","      <td>890.08443</td>\n","      <td>56.74568</td>\n","      <td>103.18358</td>\n","      <td>0.00001</td>\n","      <td>36.02044</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2793.50248</td>\n","      <td>961.56303</td>\n","      <td>1248.82314</td>\n","      <td>1333.77664</td>\n","      <td>2230.61990</td>\n","      <td>4058.13191</td>\n","      <td>0.00000</td>\n","      <td>1817.44508</td>\n","      <td>3941.76067</td>\n","      <td>2117.91820</td>\n","      <td>187.61429</td>\n","      <td>1363.29578</td>\n","      <td>1197.38200</td>\n","      <td>94.50000</td>\n","      <td>1593.49475</td>\n","      <td>1627.57344</td>\n","      <td>963.00000</td>\n","      <td>830.33288</td>\n","      <td>46.26468</td>\n","      <td>4708.87660</td>\n","      <td>0.00001</td>\n","      <td>127.33578</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>3297.37037</td>\n","      <td>900.47106</td>\n","      <td>1505.88220</td>\n","      <td>2084.60480</td>\n","      <td>1684.43836</td>\n","      <td>1952.29428</td>\n","      <td>0.00000</td>\n","      <td>749.63406</td>\n","      <td>1766.64692</td>\n","      <td>2287.93031</td>\n","      <td>193.76361</td>\n","      <td>2609.33325</td>\n","      <td>1477.11179</td>\n","      <td>76.21739</td>\n","      <td>1378.05933</td>\n","      <td>1968.12750</td>\n","      <td>1059.11111</td>\n","      <td>1199.68109</td>\n","      <td>34.59394</td>\n","      <td>890.52009</td>\n","      <td>0.00001</td>\n","      <td>390.67580</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2912.04489</td>\n","      <td>995.35110</td>\n","      <td>1378.53901</td>\n","      <td>1472.73507</td>\n","      <td>1944.49520</td>\n","      <td>1568.07465</td>\n","      <td>0.00000</td>\n","      <td>457.19890</td>\n","      <td>1667.25212</td>\n","      <td>2453.16488</td>\n","      <td>230.95000</td>\n","      <td>2587.87524</td>\n","      <td>1272.85608</td>\n","      <td>160.33333</td>\n","      <td>1568.12134</td>\n","      <td>2249.80849</td>\n","      <td>1084.85714</td>\n","      <td>907.15326</td>\n","      <td>38.35415</td>\n","      <td>783.60746</td>\n","      <td>0.00001</td>\n","      <td>10109.11512</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    90001302   90001441   90001542  ...    90001776  90001774  90001842\n","0 3226.09386 1184.94575 1416.28051  ...    28.17773   0.00000   0.00000\n","1 3011.82735 2050.50788 1238.59269  ...    36.07948   0.00000   0.00000\n","2 2794.30549  890.04308 1559.96406  ...    28.56960   0.00000   0.00000\n","3 3186.88129  809.55347 1261.85854  ...    26.11894   0.00000   0.00000\n","4 3860.56491  701.91556 1238.17586  ...    33.97936   0.00000   0.00000\n","5 2176.74400  690.30646 1140.97781  ...    13.44701   0.00000   0.00000\n","6 2773.45692  901.28155 2095.44362  ...    36.02044   0.00000   0.00000\n","7 2793.50248  961.56303 1248.82314  ...   127.33578   0.00000   0.00000\n","8 3297.37037  900.47106 1505.88220  ...   390.67580   0.00000   0.00000\n","9 2912.04489  995.35110 1378.53901  ... 10109.11512   0.00000   0.00000\n","\n","[10 rows x 24 columns]"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["shpr_pred_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAOIzsGwxhU9"},"outputs":[],"source":["shpr_pred_df.to_csv('./모델링/예측값/shpr_예측값.csv', encoding = 'utf-8', index = False)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Modeling_shpr_cd 최종.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
