# Modeling
- **5가지 머신러닝 Boosting Ensemble 기법**을 활용하여 모델링
	- XGB Regressor
	- Light GBM Regressor
	- Support Vector Regressor
	- Gradient Boosting Regressor
	- Adaboost Regressor
---
- [2021.03.01 ~ 2021.06.30]까지 **122일 간의 데이터에서 약 10%인 [2021.06.21 ~ 2021.06.30], 10일 간의 데이터셋을 test dataset**으로 설정
- **Grid search를 통해 모델별 최적의 파라미터**를 찾아내어 이를 토대로 각 모델의 학습 진행
---
- 단일 모델에서 성능 평가를 통해 MAE 값이 작은 세 개의 모델을 이후의 Stacking ensemble 모델에 사용하였으며, Stacking 모델의 meta model을 이 세 개의 모델로 변경해가며 각각의 성능을 비교하였다.
- 최종적으로 단일 모델에서의 성능이 가장 좋은 모델과 Stacking 모델에서의 성능이 가장 좋은 모델 중에 MAE 값이 작은 모델을 최종으로 선정하여 예측모델로 사용하였다.
---
1. 고객사별
	- 총 24개의 고객사 코드(SHPR_CD)별로 상품수요 예측 모델링을 진행
2. 클러스터별
	- 같은 수요 패턴을 보이는 품목끼리 클러스터를 형성하였으며, 총 10개의 클러스터별로 상품수요 예측 모델링을 진행
3. 품목별
	- 클러스터 별로 상품 수요 예측을 했지만, 품목별로 더 자세한 예측을 위해 각 클러스터에서 수요가 가장 높았던 5개의 품목을 선정하여 모델링을 진행
4. 지역별
	- 각 창고(데이터 상 GP001, KX007 두 가지 존재)에서 수취인 지역별로의 상품 수요예측 모델링을 진행

## Single Model 

- 아래의 5가지 단일 모델로 각각의 성능을 평가 (평가 지표 : MAE)

#### XGB Regressor
- 약한 분류기를 묶어서 정확도를 예측하는 기법
- Greedy Algorithm을 사용하여 분류기를 발견하고 분산처리를 사용하여 빠른 속도로 적합한 파라미터를 찾는 알고리즘
- __장점__
	- 병렬 처리를 사용하기에 학습과 분리가 빠르다
	- greedy-algorithm 을 사용한 자동 pruning가 가능하며, overfitting이 잘 일어나지 않는다

#### LGBM Regressor
- Gradient Boosting 프레임워크로 tree 기반 학습 알고리즘
- level-wise(수평 확장) 알고리즘
- 과적합에 민감하고 작은 데이터에 대해서 과적합하기 쉽기 때문에 작은 데이터 세트 사용에는 추천되지 않는다
- __장점__
	- 큰 사이즈의 데이터를 다룰 수 있고 실행시킬 때 적은 메모리 차지

#### Support Vector Regressor
- 지도 학습 머신러닝 방법 중 하나

	<img width="412" alt="스크린샷 2022-03-03 오후 3 39 43" src="https://user-images.githubusercontent.com/67430267/156510727-f2c4bdf7-e48f-4698-9236-561949b5e648.png">
	
	- 마진 내부에 데이터가 최대한 많이 들어가도록 학습하는 것
	- 마진의 폭은 epilson 이라는 하이퍼파라미터를 사용하여 조절

- **장점**
	- 서로 다른 분류에 속한 관측치 사이에 간격이 최대가 되는 선을 찾아 이것을 선으로 연결한 것으로, binary한 값이 아니라 연속된 수치로 예측이 가능

#### Gradient Boosting Regressor
- 여러 개의 decision tree를 묶어 강력한 model을 만드는 ensemble 기법
- 앙상블 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가하지만 이전 예측기가 만든 잔여오차에 새로운 예측기를 학습  
- __장점__
	- 무작위성이 없어 powerful한 pre-pruning이 사용되며 1~5 정도 깊이의 tree를 사용하므로 메모리를 적게 사용하고 예측도 빠르다
	- 파라미터 설정에 조금 더 민감하지만 잘 조정하면 높은 정확도를 제공

#### Adaboost Regressor
- Adaptive(잘 분류해내지 못한 데이터들의 가중치를 adaptive하게 바꿔가며 학습이 더 잘되게함) + Boosting
- 약한 분류기(weak classifier)들이 상호보완 하도록 순차적(sequential)으로 학습하고, 이들을 조합하여 최종적으로 강한 분류기(strong classifier)의 성능을 향상시키는 것
	- 예측 성능이 낮은 약한 분류기들을 조합하여 최종적으로 조금 더 성능이 좋은 강한 분류기 하나를 만드는 것
	- 약한 분류기들이 상호보완적(adaptive)으로 학습해나가고, 이러한 약한 분류기들을 조합하여 하나의 분류기를 만든다.

## Stacking Model
- 여러 모델들을 활용해 각각의 예측 결과를 도출한 뒤 그 예측 결과를 결합해 최종 예측 결과를 만들어내는 것

- 스태킹 알고리즘에는  **총 2가지 단계**가 존재

	**단계 1.**  n개의 모델로 학습 데이터로 학습 모델 생성
	**단계 2.**  n개의 모델에서 학습을 마친 뒤 예측한 값들을 합쳐서 최종 예측

	<img width="667" alt="스크린샷 2022-03-03 오후 4 06 37" src="https://user-images.githubusercontent.com/67430267/156513995-e6a24da9-5c33-4943-bc90-f16c8f772d0b.png">

	- 위에서 설명한 바와 같이 단일 모델에서 성능 평가를 통해 MAE 값이 작은 세 개의 모델로 학습 모델을 생성하였다.

## Result

<img width="634" alt="KakaoTalk_Photo_2022-03-03-23-00-37" src="https://user-images.githubusercontent.com/67430267/156579621-ccf4d0d5-115e-4d41-832e-321dda194170.png">

- 위 그림은 학습된 모델의 예측값 일부만을 나타내었다
- 좌측은 **고객사별**, 우측은 **지역별**로의 모델링 결과이다
- 주황색이 **실제값**, 초록색이 **예측값**을 나타낸다
