{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"shpr_cd_max 모델링","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"mlcxtWEjUjwS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635453112939,"user_tz":-540,"elapsed":54014,"user":{"displayName":"[20_HG006Y]석민정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13174376527075178770"}},"outputId":"874f1ce4-9b00-4de5-e82b-4127c308c7b7"},"source":["from google.colab import auth\n","auth.authenticate_user()\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X11eBgt8m_7G","executionInfo":{"status":"ok","timestamp":1635453148350,"user_tz":-540,"elapsed":28133,"user":{"displayName":"[20_HG006Y]석민정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13174376527075178770"}},"outputId":"d30cb03b-896c-40aa-88c9-c0a2f74804ed"},"source":["# download sktime package \n","!pip install sktime"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sktime\n","  Downloading sktime-0.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.1 MB)\n","\u001b[K     |████████████████████████████████| 6.1 MB 4.8 MB/s \n","\u001b[?25hRequirement already satisfied: statsmodels<=0.12.1 in /usr/local/lib/python3.7/dist-packages (from sktime) (0.10.2)\n","Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.19.5)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from sktime) (0.37.0)\n","Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.1.5)\n","Collecting statsmodels>=0.12.1\n","  Downloading statsmodels-0.13.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n","\u001b[K     |████████████████████████████████| 9.8 MB 47.8 MB/s \n","\u001b[?25hCollecting numba>=0.53\n","  Downloading numba-0.54.1-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 28.7 MB/s \n","\u001b[?25hCollecting scikit-learn>=0.24.0\n","  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n","\u001b[K     |████████████████████████████████| 23.2 MB 1.3 MB/s \n","\u001b[?25hCollecting llvmlite<0.38,>=0.37.0rc1\n","  Downloading llvmlite-0.37.0-cp37-cp37m-manylinux2014_x86_64.whl (26.3 MB)\n","\u001b[K     |████████████████████████████████| 26.3 MB 92 kB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->sktime) (57.4.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->sktime) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->sktime) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->sktime) (1.15.0)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->sktime) (1.0.1)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->sktime) (1.4.1)\n","Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.12.1->sktime) (0.5.2)\n","Installing collected packages: threadpoolctl, llvmlite, statsmodels, scikit-learn, numba, sktime\n","  Attempting uninstall: llvmlite\n","    Found existing installation: llvmlite 0.34.0\n","    Uninstalling llvmlite-0.34.0:\n","      Successfully uninstalled llvmlite-0.34.0\n","  Attempting uninstall: statsmodels\n","    Found existing installation: statsmodels 0.10.2\n","    Uninstalling statsmodels-0.10.2:\n","      Successfully uninstalled statsmodels-0.10.2\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","  Attempting uninstall: numba\n","    Found existing installation: numba 0.51.2\n","    Uninstalling numba-0.51.2:\n","      Successfully uninstalled numba-0.51.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","sktime 0.8.1 requires statsmodels<=0.12.1, but you have statsmodels 0.13.0 which is incompatible.\u001b[0m\n","Successfully installed llvmlite-0.37.0 numba-0.54.1 scikit-learn-1.0.1 sktime-0.8.1 statsmodels-0.13.0 threadpoolctl-3.0.0\n"]}]},{"cell_type":"code","metadata":{"id":"aqS4t0ovUqTb"},"source":["# 필요한 패키지 import\n","import os\n","import sys\n","import warnings\n","import plotly\n","import numpy as np\n","import pandas as pd\n","import datetime\n","import tensorflow as tf\n","from tqdm import tqdm\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import KFold, train_test_split, TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from sklearn import preprocessing\n","from sklearn.preprocessing import RobustScaler, StandardScaler\n","from sktime.forecasting.model_selection import temporal_train_test_split\n","from sktime.utils.plotting import plot_series\n","from scipy.stats import reciprocal \n","\n","from xgboost import XGBRegressor\n","from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n","from sklearn.svm import SVR\n","import lightgbm as lgb\n","from lightgbm.sklearn import LGBMRegressor\n","from sklearn.multioutput import MultiOutputRegressor\n","\n","mpl.rcParams['figure.figsize'] = (8, 6)\n","mpl.rcParams['axes.grid'] = False\n","\n","warnings.filterwarnings('ignore')\n","pd.options.display.float_format = '{:.5f}'.format"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C9ZVn4eEn9iq","executionInfo":{"status":"ok","timestamp":1635453164238,"user_tz":-540,"elapsed":1106,"user":{"displayName":"[20_HG006Y]석민정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13174376527075178770"}},"outputId":"20f0642f-eac3-42ae-817d-a1a00a4a5435"},"source":["%cd /content/gdrive/Shareddrives/cj공모전/최종코드/모델링/shpr_max"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/Shareddrives/cj공모전/최종코드/모델링/shpr_max\n"]}]},{"cell_type":"code","metadata":{"id":"6x6993ZHqvAf"},"source":["shpr_df = ['90001702', '90001705']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YXYZvSEhoFNQ"},"source":["# 학습에 필요한 train 및 test dataset 만드는 과정\n","def get_train_test_set(shpr_cd):\n","  df = pd.read_pickle(\"item_cd_\"+shpr_cd+\".pkl\")\n","  train = df[df['BKG_DATE'] <= '2021-06-20']\n","  test = df[df['BKG_DATE'] > '2021-06-20']\n","  # 나머지 Scaling\n","  scaling_features = ['요일', '휴일여부', 'DAY_1', 'DAY_2', 'DAY_3', 'WEEK_AMT', 'MEAN_PRICE',\n","       '강원', '경기', '경남', '경북', '광주', '대구', '대전', '부산', '서울', '세종', '울산', '인천',\n","       '전남', '전북', '제주', '충남', '충북']\n","\n","  scaler = StandardScaler()\n","  train.loc[:, scaling_features] = scaler.fit_transform(train[scaling_features])\n","  test.loc[:, scaling_features] = scaler.transform(test[scaling_features])\n","  train_y = train['ITEM_QTY']\n","  train_x = train.drop(['ITEM_QTY','BKG_DATE', 'week'], axis=1)\n","\n","  test_y = test['ITEM_QTY']\n","  test_x = test.drop(['ITEM_QTY','BKG_DATE', 'week'], axis=1)\n","\n","  \n","  return train_x, train_y, test_x, test_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fw_LCffCKoVE"},"source":["# 모델별 학습 파라미터 정의\n","XGBRegressor_param = {'n_estimators' : [100], 'eta' : [0.01], 'min_child_weight' : np.arange(1, 8, 1), 'max_depth' : np.arange(3,9,1) , 'colsample_bytree' :np.arange(0.8, 1.0, 0.1), 'subsample' :np.arange(0.8, 1.0, 0.1)}\n","LGBMRegressor_param = {'max_depth' : range(3,15,3), 'min_child_weight': range(1,6,2), 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100], 'learning_rate':[0.1, 0.01], 'max_depth' : [6,8,10]}\n","SVR_param = {'kernel':['linear'], 'C':[1.0], 'epsilon':[0.1]}\n","GradientBoostingRegressor_param = {'n_estimators':[100], 'max_depth':np.arange(3,20,3)}\n","AdaBoostRegressor_param = {'n_estimators' : np.arange(25, 100, 25), 'loss': ['linear', 'square', 'exponential'], 'learning_rate': np.arange(0.1, 1)} "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7_YiraP3Lp5F"},"source":["# 모델별 정의\n","XGBRegressor_model = XGBRegressor(n_estimators = 100, objective = 'reg:squarederror')\n","LGBMRegressor_model = LGBMRegressor(n_estimators = 80)\n","SVR_model = SVR(kernel='linear', C=1.0, epsilon=0.1)\n","GradientBoostingRegressor_model = GradientBoostingRegressor(n_estimators=100, max_depth=3)\n","AdaBoostRegressor_model = AdaBoostRegressor(base_estimator=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xOwgwkfxOntj"},"source":["# 단일 모델별 최적 파라미터로 모델링\n","def print_best_params(model, params, x_train, x_test, y_train, y_test, log=False):\n","\n","  tss = TimeSeriesSplit(n_splits=5)\n","  grid_model=GridSearchCV(model, cv = tss, param_grid=params, scoring='neg_mean_absolute_error')\n","  grid_model.fit(x_train, y_train)\n","  mae = -1 * grid_model.best_score_\n","  #print('{0} 최적 평균 mae값 : {1}, 최적 파라미터:{2}'.format(model.__class__.__name__, np.round(mae, 4), grid_model.best_params_))\n","\n","  best_model=grid_model.best_estimator_\n","  pred=best_model.predict(x_test)\n","\n","  if log:\n","    y_test=np.expm1(y_test)\n","    pred=np.expm1(pred)\n","  \n","  single_min_list = np.round(mean_absolute_error(y_test, pred), 4)\n","\n","  return best_model, single_min_list, pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UoKJha9-U3fw"},"source":["# 단일 모델에서의 MAE 값이 가장 작은 세 개의 모델로 stacking, stacking model의 dataset 만드는 함수\n","def get_stacking_base_datasets(model, x_train_n, y_train_n, x_test_n, n_splits=5):\n","  # 지정된 n_folds 값으로 KFold 생성\n","  tss = TimeSeriesSplit(n_splits)\n","\n","  # 추후 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화\n","  train_fold_pred=np.zeros((x_train_n.shape[0], 1))\n","  test_pred=np.zeros((x_test_n.shape[0], n_splits))\n","  #print(model.__class__.__name__, ' model 시작')\n","\n","  for folder_counter, (train_index, valid_index) in enumerate(tss.split(x_train_n)):\n","    # 입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 세트 추출\n","    #print('\\t 폴드 세트: ', folder_counter, ' 시작')\n","    x_tr=x_train_n[train_index]\n","    y_tr=y_train_n[train_index]\n","    x_te=x_train_n[valid_index]\n","\n","    # 폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행\n","    model.fit(x_tr, y_tr)\n","    # 폴드 세트 내부에서 다시 만들어지 검증 데이터로 기반 모델 예측 후 데이터 저장\n","    train_fold_pred[valid_index, :]=model.predict(x_te).reshape(-1, 1)\n","    # 입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장\n","    test_pred[:, folder_counter]=model.predict(x_test_n)\n","\n","  # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성\n","  test_pred_mean=np.mean(test_pred, axis=1).reshape(-1, 1)\n","\n","  # train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터\n","  return train_fold_pred, test_pred_mean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"betSsoZEFiV3"},"source":["import sys\n","mod = sys.modules[__name__]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IBoibHKPO26R"},"source":["\n","def get_optimal_model(shpr_cd):\n","  x_train, y_train, x_test, y_test = get_train_test_set(shpr_cd)\n","  x_train_n=x_train.values\n","  x_test_n=x_test.values\n","  y_train_n=y_train.values\n","  \n","  single_min_list = dict()\n","  \n","  # 단일 모델 학습\n","  globals()[\"XGBRegressor_model_tuned_{}\".format(shpr_cd)], single_min_list[\"XGBRegressor\"], globals()[\"single_pred_XGBRegressor_{}\".format(shpr_cd)] = print_best_params(XGBRegressor_model, XGBRegressor_param, x_train, x_test, y_train, y_test)\n","  globals()[\"LGBMRegressor_model_tuned_{}\".format(shpr_cd)], single_min_list[\"LGBMRegressor\"], globals()[\"single_pred_LGBMRegressor_{}\".format(shpr_cd)] = print_best_params(LGBMRegressor_model, LGBMRegressor_param, x_train, x_test, y_train, y_test)\n","  globals()[\"SVR_model_tuned_{}\".format(shpr_cd)], single_min_list[\"SVR\"], globals()[\"single_pred_SVR_{}\".format(shpr_cd)] = print_best_params(SVR_model, SVR_param, x_train, x_test, y_train, y_test)\n","  globals()[\"GradientBoostingRegressor_model_tuned_{}\".format(shpr_cd)], single_min_list[\"GradientBoostingRegressor\"], globals()[\"single_pred_GradientBoostingRegressor_{}\".format(shpr_cd)] = print_best_params(GradientBoostingRegressor_model, GradientBoostingRegressor_param, x_train, x_test, y_train, y_test)\n","  globals()[\"AdaBoostRegressor_model_tuned_{}\".format(shpr_cd)], single_min_list[\"AdaBoostRegressor\"], globals()[\"single_pred_AdaBoostRegressor_{}\".format(shpr_cd)] = print_best_params(AdaBoostRegressor_model, AdaBoostRegressor_param, x_train, x_test, y_train, y_test)\n","\n","  single_model_mae = sorted(single_min_list.items(), key = lambda item: item[1])\n","  \n","  stacking_list = dict()\n","\n","# stacking model dataset 생성\n","  globals()[\"{}_train_{}\".format(single_model_mae[0][0], shpr_cd)], globals()[\"{}_test_{}\".format(single_model_mae[0][0], shpr_cd)] = get_stacking_base_datasets(getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[0][0], shpr_cd)), x_train_n, y_train_n, x_test_n, 5)                                                                                                          \n","  globals()[\"{}_train_{}\".format(single_model_mae[1][0], shpr_cd)], globals()[\"{}_test_{}\".format(single_model_mae[1][0], shpr_cd)] = get_stacking_base_datasets(getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[1][0], shpr_cd)), x_train_n, y_train_n, x_test_n, 5) \n","                                                                                                            \n","  # 첫번째 경우\n","  stack_final_x_train = np.concatenate((getattr(mod, \"{}_train_{}\".format(single_model_mae[0][0], shpr_cd)), getattr(mod, \"{}_train_{}\".format(single_model_mae[1][0], shpr_cd))), axis=1)\n","  stack_final_x_test = np.concatenate((getattr(mod, \"{}_test_{}\".format(single_model_mae[0][0], shpr_cd)), getattr(mod, \"{}_test_{}\".format(single_model_mae[1][0], shpr_cd))), axis=1)\n","\n","  globals()[\"meta_model_{}_{}\".format(single_model_mae[2][0], shpr_cd)] = getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[2][0], shpr_cd))\n","\n","  getattr(mod, \"meta_model_{}_{}\".format(single_model_mae[2][0], shpr_cd)).fit(stack_final_x_train, y_train)\n","  globals()[\"stack_pred_{}_{}\".format(single_model_mae[2][0], shpr_cd)] = getattr(mod, \"meta_model_{}_{}\".format(single_model_mae[2][0], shpr_cd)).predict(stack_final_x_test)\n","  stacking_list[\"{}\".format(single_model_mae[2][0])] =  mean_absolute_error(y_test, getattr(mod, \"stack_pred_{}_{}\".format(single_model_mae[2][0], shpr_cd)))\n","\n","  # 두번째 경우\n","  globals()[\"{}_train_{}\".format(single_model_mae[0][0], shpr_cd)], globals()[\"{}_test_{}\".format(single_model_mae[0][0], shpr_cd)] = get_stacking_base_datasets(getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[0][0], shpr_cd)), x_train_n, y_train_n, x_test_n, 5)\n","                                                                                                            \n","  globals()[\"{}_train_{}\".format(single_model_mae[2][0], shpr_cd)], globals()[\"{}_test_{}\".format(single_model_mae[2][0], shpr_cd)] = get_stacking_base_datasets(getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[2][0], shpr_cd)), x_train_n, y_train_n, x_test_n, 5) \n","                                                                                                            \n","\n","  stack_final_x_train = np.concatenate((getattr(mod, \"{}_train_{}\".format(single_model_mae[0][0], shpr_cd)), getattr(mod, \"{}_train_{}\".format(single_model_mae[2][0], shpr_cd))), axis=1)\n","  stack_final_x_test = np.concatenate((getattr(mod, \"{}_test_{}\".format(single_model_mae[0][0], shpr_cd)), getattr(mod, \"{}_test_{}\".format(single_model_mae[2][0], shpr_cd))), axis=1)\n","\n","  globals()[\"meta_model_{}_{}\".format(single_model_mae[1][0], shpr_cd)] = getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[1][0], shpr_cd))\n","\n","  getattr(mod, \"meta_model_{}_{}\".format(single_model_mae[1][0], shpr_cd)).fit(stack_final_x_train, y_train)\n","  globals()[\"stack_pred_{}_{}\".format(single_model_mae[1][0], shpr_cd)] = getattr(mod, \"meta_model_{}_{}\".format(single_model_mae[1][0], shpr_cd)).predict(stack_final_x_test)\n","  stacking_list[\"{}\".format(single_model_mae[1][0])] =  mean_absolute_error(y_test, getattr(mod, \"stack_pred_{}_{}\".format(single_model_mae[1][0], shpr_cd)))\n","\n","\n","  # 세번째 경우\n","  globals()[\"{}_train_{}\".format(single_model_mae[1][0], shpr_cd)], globals()[\"{}_test_{}\".format(single_model_mae[1][0], shpr_cd)] = get_stacking_base_datasets(getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[1][0], shpr_cd)), x_train_n, y_train_n, x_test_n, 5)\n","                                                                                                            \n","  globals()[\"{}_train_{}\".format(single_model_mae[2][0], shpr_cd)], globals()[\"{}_test_{}\".format(single_model_mae[2][0], shpr_cd)] = get_stacking_base_datasets(getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[2][0], shpr_cd)), x_train_n, y_train_n, x_test_n, 5) \n","                                                                                                            \n","\n","  stack_final_x_train = np.concatenate((getattr(mod, \"{}_train_{}\".format(single_model_mae[1][0], shpr_cd)), getattr(mod, \"{}_train_{}\".format(single_model_mae[2][0], shpr_cd))), axis=1)\n","  stack_final_x_test = np.concatenate((getattr(mod, \"{}_test_{}\".format(single_model_mae[1][0], shpr_cd)), getattr(mod, \"{}_test_{}\".format(single_model_mae[2][0], shpr_cd))), axis=1)\n","\n","  globals()[\"meta_model_{}_{}\".format(single_model_mae[0][0], shpr_cd)] = getattr(mod, \"{}_model_tuned_{}\".format(single_model_mae[0][0], shpr_cd))\n","\n","  getattr(mod, \"meta_model_{}_{}\".format(single_model_mae[0][0], shpr_cd)).fit(stack_final_x_train, y_train)\n","  globals()[\"stack_pred_{}_{}\".format(single_model_mae[0][0], shpr_cd)] = getattr(mod, \"meta_model_{}_{}\".format(single_model_mae[0][0], shpr_cd)).predict(stack_final_x_test)\n","  stacking_list[\"{}\".format(single_model_mae[0][0])] =  mean_absolute_error(y_test, getattr(mod, \"stack_pred_{}_{}\".format(single_model_mae[0][0], shpr_cd)))\n","\n","  stacking_model_mae = sorted(stacking_list.items(), key = lambda item: item[1])\n","\n","# 단일 모델과 stacking 모델의 MAE 값을 비교하여 작은 값으로 모델링 결과 반환\n","  a = single_model_mae[0][1]\n","  b = stacking_model_mae[0][1]\n","\n","  if a < b:\n","    print(\"쇼핑몰 코드 {}의 최적 모델은 단일 모델 {} : (test MAE값) {}\".format(shpr_cd, single_model_mae[0][0], single_model_mae[0][1]))\n","    globals()[\"best_model_{}\".format(shpr_cd)]  = single_model_mae[0][0]\n","    globals()[\"test_mae_{}\".format(shpr_cd)] = single_model_mae[0][1]\n","    globals()[\"best_pred_{}\".format(shpr_cd)] = getattr(mod, \"single_pred_{}_{}\".format(single_model_mae[0][0], shpr_cd))\n","  else:\n","    print(\"쇼핑몰 코드 {}의 최적 모델은 stacking meta 모델 {} : (test MAE값) {}\".format(shpr_cd, stacking_model_mae[0][0], stacking_model_mae[0][1]))\n","    globals()[\"best_model_{}\".format(shpr_cd)] = stacking_model_mae[0][0]\n","    globals()[\"test_mae_{}\".format(shpr_cd)] = stacking_model_mae[0][1]\n","    globals()[\"best_pred_{}\".format(shpr_cd)] = getattr(mod, \"stack_pred_{}_{}\".format(stacking_model_mae[0][0], shpr_cd))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LX9-RARbgAEv","executionInfo":{"status":"ok","timestamp":1635453212834,"user_tz":-540,"elapsed":35840,"user":{"displayName":"[20_HG006Y]석민정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13174376527075178770"}},"outputId":"c800a5f8-cf4c-4c80-cc77-b909f8c296d4"},"source":["# 단일 모델과 stacking 모델의 MAE 값을 비교하여 작은 값으로 모델링 결과 반환\n","for shpr_cd in shpr_df:\n","  get_optimal_model(str(shpr_cd))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["쇼핑몰 코드 90001702의 최적 모델은 단일 모델 SVR : (test MAE값) 1.1246\n","쇼핑몰 코드 90001705의 최적 모델은 단일 모델 SVR : (test MAE값) 0.5524\n"]}]},{"cell_type":"code","metadata":{"id":"lpk5wzBXjcfx"},"source":["for shpr_cd in shpr_df:\n","  x_train, y_train, x_test, y_test = get_train_test_set(shpr_cd)\n","  globals()['pred_{}'.format(shpr_cd)]= getattr(mod, \"stack_pred_{}_{}\".format(getattr(mod, \"best_model_{}\".format(shpr_cd)), shpr_cd))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nf4y3SqpoyVr"},"source":["# test dataset에 대한 예측값 \n","shpr_pred_df = pd.DataFrame()\n","for shpr_cd in shpr_df:\n","  shpr_pred_df[str(shpr_cd)] = getattr(mod, 'best_pred_{}'.format(shpr_cd))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8srdGc-yxfor","colab":{"base_uri":"https://localhost:8080/","height":362},"executionInfo":{"status":"ok","timestamp":1635453222087,"user_tz":-540,"elapsed":7,"user":{"displayName":"[20_HG006Y]석민정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13174376527075178770"}},"outputId":"96ba1152-4b26-4867-8d04-e9f3abff1344"},"source":["shpr_pred_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>90001702</th>\n","      <th>90001705</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.16734</td>\n","      <td>2.86213</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.09692</td>\n","      <td>0.35961</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5.58590</td>\n","      <td>0.00567</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.51389</td>\n","      <td>6.79639</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.47510</td>\n","      <td>1.34246</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2.70108</td>\n","      <td>-0.15893</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2.31776</td>\n","      <td>1.80235</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>11.56999</td>\n","      <td>3.87486</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1.16899</td>\n","      <td>1.46049</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1.52973</td>\n","      <td>2.74834</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   90001702  90001705\n","0   1.16734   2.86213\n","1   2.09692   0.35961\n","2   5.58590   0.00567\n","3   2.51389   6.79639\n","4   2.47510   1.34246\n","5   2.70108  -0.15893\n","6   2.31776   1.80235\n","7  11.56999   3.87486\n","8   1.16899   1.46049\n","9   1.52973   2.74834"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"fAOIzsGwxhU9"},"source":["shpr_pred_df.to_csv('/content/gdrive/Shareddrives/cj공모전/최종코드/모델링/예측값/shpr_max_예측값.csv', encoding = 'utf-8', index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zAPpNn-Hx179"},"source":[""],"execution_count":null,"outputs":[]}]}